{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roixiao/x-research-skill/blob/main/whisperf_colab_FINAL_20260210_225215.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtG8l2t25Pd_"
      },
      "source": [
        "# Whisperf (Colab) — Batch Download + Batch Transcribe (FINAL (Colab))\n",
        "\n",
        "这份 notebook 专门为 **Google Colab / Colab Pro** 重新设计：\n",
        "\n",
        "- 输出按 run 分目录：`/content/whisperf/runs/<RUN_ID>/...`\n",
        "- 输入来源：\n",
        "  - Step 1: `yt-dlp` 批量下载（公开视频通常不需要 cookie；会员/私密才需要 cookie）\n",
        "  - Step 1-ALT: 上传音频或从 Google Drive 路径导入\n",
        "- 输出：`txt / srt / json` + `task_log.txt` + `download_logs/`，最后打包 zip 并下载\n",
        "\n",
        "## 注意\n",
        "- members-only / 私密视频需要你自己的 `cookies.txt`（Netscape 格式）。\n",
        "- 即使是公开视频，也可能被站点风控导致拿不到格式（只剩 storyboard）。本 notebook 会记录诊断日志，但不能保证 100% 下载成功。\n",
        "- 多 GPU：Colab 常见只有 1 张 GPU。本 notebook 支持“多卡时按文件并行”。\n",
        "\n",
        "---\n"
      ],
      "id": "OtG8l2t25Pd_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kIwWynh5PeB",
        "outputId": "30c53e2b-13e4-4faa-ddb9-2568bdd8ad75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "node: /usr/bin/node\n",
            "v12.22.9\n",
            "2026.02.09.233747\n",
            "Setup complete\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Step 0: Setup (Colab)\n",
        "# ============================================================\n",
        "\n",
        "# YouTube 风控变化：建议装 yt-dlp[default] + yt-dlp-ejs，并确保 node 可用。\n",
        "\n",
        "!pip -q install -U --pre \"yt-dlp[default]\" yt-dlp-ejs openai-whisper tqdm\n",
        "\n",
        "!apt-get -qq update > /dev/null\n",
        "!apt-get -qq install -y ffmpeg nodejs npm > /dev/null 2>&1 || true\n",
        "\n",
        "# Ubuntu 有时只有 nodejs 没有 node，这里做一个兼容链接。\n",
        "!bash -lc 'set -e; if ! command -v node >/dev/null 2>&1; then if command -v nodejs >/dev/null 2>&1; then ln -sf \"$(command -v nodejs)\" /usr/local/bin/node; fi; fi; true'\n",
        "\n",
        "# Self-check\n",
        "!bash -lc 'echo \"node: $(command -v node || true)\"; node -v || true'\n",
        "!python -m yt_dlp --version\n",
        "\n",
        "print('Setup complete')\n"
      ],
      "id": "6kIwWynh5PeB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7cef6b3",
        "outputId": "40de3d2f-9af4-4ae1-96e3-352d639d7478"
      },
      "source": [
        "print(f\"Source URL: {urls[0]}\")\n",
        "print(f\"Local Path: {ok[0]['path']}\")"
      ],
      "id": "a7cef6b3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source URL: https://youtu.be/dbV1sMfv9TQ?si=v2YlYUGoovCJTiRn\n",
            "Local Path: /content/whisperf/runs/20260211_041443/audio/youtube_20260211_041922/Joe Tsai, Co-Founder and Chairman, Alibaba： Find Your People_dbV1sMfv9TQ.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZKONE-O5PeC",
        "outputId": "45cbe5cc-1133-4225-8b3b-151893d891df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2026-02-11 04:24:12] [INFO] [init] run_id=20260211_042412 run_dir=/content/whisperf/runs/20260211_042412 gpus=1\n",
            "RUN_DIR: /content/whisperf/runs/20260211_042412\n",
            "AUDIO_DIR: /content/whisperf/runs/20260211_042412/audio\n",
            "OUT_DIR: /content/whisperf/runs/20260211_042412/output\n",
            "DL_LOG_DIR: /content/whisperf/runs/20260211_042412/download_logs\n",
            "TASK_LOG: /content/whisperf/runs/20260211_042412/task_log.txt\n",
            "GPU 0: NVIDIA A100-SXM4-80GB\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Step 0b: Run Directories + Logging + GPU Info\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os, re, sys, json, glob, time, shutil, zipfile, subprocess, hashlib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import traceback\n",
        "\n",
        "import torch\n",
        "\n",
        "RUN_ID = datetime.now().strftime('%Y%m%d_%H%M%S')  #@param {type:\"string\"}\n",
        "BASE_DIR = '/content/whisperf'  #@param {type:\"string\"}\n",
        "\n",
        "RUN_DIR = Path(BASE_DIR) / 'runs' / RUN_ID\n",
        "AUDIO_DIR = RUN_DIR / 'audio'\n",
        "OUT_DIR = RUN_DIR / 'output'\n",
        "DL_LOG_DIR = RUN_DIR / 'download_logs'\n",
        "\n",
        "for d in [RUN_DIR, AUDIO_DIR, OUT_DIR, DL_LOG_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "COOKIE_FILE = RUN_DIR / 'cookies.txt'\n",
        "TASK_LOG_PATH = RUN_DIR / 'task_log.txt'\n",
        "\n",
        "AUDIO_EXTS = {'.mp3', '.m4a', '.wav', '.flac', '.opus', '.ogg', '.aac', '.webm', '.mp4', '.mkv'}\n",
        "\n",
        "\n",
        "def _ts():\n",
        "    return datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "\n",
        "def safe_name(name: str) -> str:\n",
        "    name = re.sub(r'[\\/:*?\"<>|]+', '_', name)\n",
        "    return re.sub(r'\\s+', ' ', name).strip()[:180]\n",
        "\n",
        "\n",
        "def _log_write(line: str):\n",
        "    try:\n",
        "        with open(TASK_LOG_PATH, 'a', encoding='utf-8') as f:\n",
        "            f.write(line + '\\n')\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "def log_event(step: str, message: str, level: str = 'INFO'):\n",
        "    ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    line = f'[{ts}] [{level}] [{step}] {message}'\n",
        "    _log_write(line)\n",
        "    print(line)\n",
        "\n",
        "\n",
        "def log_exception(step: str, where: str, exc: Exception):\n",
        "    ts = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    header = f'[{ts}] [ERROR] [{step}] {where}: {repr(exc)}'\n",
        "    _log_write(header)\n",
        "    _log_write(traceback.format_exc())\n",
        "    print(header)\n",
        "\n",
        "\n",
        "NUM_GPUS = torch.cuda.device_count()\n",
        "log_event('init', f'run_id={RUN_ID} run_dir={RUN_DIR} gpus={NUM_GPUS}')\n",
        "print('RUN_DIR:', RUN_DIR)\n",
        "print('AUDIO_DIR:', AUDIO_DIR)\n",
        "print('OUT_DIR:', OUT_DIR)\n",
        "print('DL_LOG_DIR:', DL_LOG_DIR)\n",
        "print('TASK_LOG:', TASK_LOG_PATH)\n",
        "\n",
        "if NUM_GPUS:\n",
        "    for i in range(NUM_GPUS):\n",
        "        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')\n",
        "else:\n",
        "    print('No GPU detected (CPU mode will be very slow)')"
      ],
      "id": "cZKONE-O5PeC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SmDaZVf5PeD"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# (Optional) Mount Google Drive (Persistence)\n",
        "# ============================================================\n",
        "\n",
        "# 如果你希望关机/断线后也不丢结果，建议把最终 zip 复制到 Drive。\n",
        "# 挂载会弹出授权窗口，这是 Colab 正常流程。\n",
        "\n",
        "MOUNT_DRIVE = True  #@param {type:\"boolean\"}\n",
        "DRIVE_BASE_DIR = '/content/drive/MyDrive/whisperf_runs'  #@param {type:\"string\"}\n",
        "\n",
        "DRIVE_RUN_DIR = None\n",
        "\n",
        "if MOUNT_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_RUN_DIR = Path(DRIVE_BASE_DIR) / RUN_ID\n",
        "    DRIVE_RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    print('Drive dir:', DRIVE_RUN_DIR)\n"
      ],
      "id": "5SmDaZVf5PeD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH69k7UQ5PeD",
        "outputId": "c022d976-8536-4800-b374-68f5d2e9ebfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cookies disabled\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Step Cookie (Optional): Provide cookies.txt for members-only\n",
        "# ============================================================\n",
        "\n",
        "# USE_COOKIES=False 时，不会传 --cookies。\n",
        "# COOKIE_PATH 可填 Drive 路径，例如:\n",
        "#   /content/drive/MyDrive/whisperf/cookies.txt\n",
        "# 留空且 USE_COOKIES=True 会弹出上传窗口。\n",
        "\n",
        "USE_COOKIES = False  #@param {type:\"boolean\"}\n",
        "COOKIE_PATH = ''  #@param {type:\"string\"}\n",
        "\n",
        "if USE_COOKIES:\n",
        "    from google.colab import files\n",
        "\n",
        "    src = COOKIE_PATH.strip()\n",
        "    if src:\n",
        "        p = Path(src)\n",
        "        if not p.exists():\n",
        "            raise FileNotFoundError(f'COOKIE_PATH not found: {p}')\n",
        "        COOKIE_FILE.write_text(p.read_text(encoding='utf-8', errors='replace'), encoding='utf-8')\n",
        "        log_event('cookie', f'copied from {p} -> {COOKIE_FILE}')\n",
        "    else:\n",
        "        up = files.upload()\n",
        "        if not up:\n",
        "            raise RuntimeError('no cookie file uploaded')\n",
        "        name, data = next(iter(up.items()))\n",
        "        COOKIE_FILE.write_bytes(data)\n",
        "        log_event('cookie', f'uploaded {name} -> {COOKIE_FILE}')\n",
        "\n",
        "    n = sum(1 for l in COOKIE_FILE.read_text(encoding='utf-8', errors='replace').splitlines() if l.strip() and not l.startswith('#'))\n",
        "    print(f'cookie lines: {n}  path: {COOKIE_FILE}')\n",
        "else:\n",
        "    print('cookies disabled')\n"
      ],
      "id": "LH69k7UQ5PeD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VuzJEf15PeD",
        "outputId": "8c42ca7c-e216-4f2f-bc55-edfc38b9a0c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2026-02-11 04:37:00] [INFO] [download] urls=1 mode=direct fmt=mp3 q=5 par=2 N=8 cookie=False\n",
            "output: /content/whisperf/runs/20260211_042412/audio/youtube_20260211_043700\n",
            "CMD: /usr/bin/python3 -m yt_dlp --no-playlist --newline --socket-timeout 30 --retries 15 --fragment-retries 15 --extractor-retries 5 --retry-sleep http:exp=1:20 --http-chunk-size 10M -N 8 -f bestaudio/best -o /content/whisperf/runs/20260211_042412/audio/youtube_20260211_043700/%(title).150s_%(id)s.%(ext)s --force-ipv4 --js-runtimes node --extractor-args youtube:player_client=web,android https://youtu.be/dbV1sMfv9TQ?si=v2YlYUGoovCJTiRn\n",
            "OK https://youtu.be/dbV1sMfv9TQ?si=v2YlYUGoovCJTiRn\n",
            "\n",
            "Done: 1/1 ok\n",
            "  OK: /content/whisperf/runs/20260211_042412/audio/youtube_20260211_043700/Joe Tsai, Co-Founder and Chairman, Alibaba： Find Your People_dbV1sMfv9TQ.mp4\n",
            "[2026-02-11 04:37:06] [INFO] [download] done ok=1 total=1 out=/content/whisperf/runs/20260211_042412/audio/youtube_20260211_043700\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Step 1: Batch Download (yt-dlp)\n",
        "# ============================================================\n",
        "\n",
        "DO_DOWNLOAD = True  #@param {type:\"boolean\"}\n",
        "\n",
        "URLS = 'https://youtu.be/dbV1sMfv9TQ?si=v2YlYUGoovCJTiRn'  #@param {type:\"string\"}\n",
        "\n",
        "OUT_MODE = 'direct'  #@param ['direct', 'extract']\n",
        "OUT_FMT = 'mp3'  #@param ['mp3', 'm4a', 'wav', 'flac', 'opus']\n",
        "OUT_QUALITY = 5  #@param {type:\"integer\"}\n",
        "URL_PARALLEL = 2  #@param {type:\"integer\"}\n",
        "N_CONNECTIONS = 8  #@param {type:\"integer\"}\n",
        "FORCE_IPV4 = True  #@param {type:\"boolean\"}\n",
        "SHOW_CMD = True  #@param {type:\"boolean\"}\n",
        "\n",
        "PLAYER_CLIENT = 'auto'  #@param ['auto', 'web,android', 'android', 'ios', 'tv_embedded,web', 'default']\n",
        "\n",
        "DIAG_FORMATS_ONLY = False  #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "def _js_runtime_auto() -> str:\n",
        "    import shutil\n",
        "    return 'node' if shutil.which('node') else ('nodejs' if shutil.which('nodejs') else '')\n",
        "\n",
        "\n",
        "def _looks_like_blocked(log: str) -> bool:\n",
        "    s = (log or '').lower()\n",
        "    needles = [\n",
        "        'n challenge',\n",
        "        'only images are available',\n",
        "        'requested format is not available',\n",
        "        'sign in',\n",
        "        'members-only',\n",
        "        'join this channel',\n",
        "        'http error 403',\n",
        "    ]\n",
        "    return any(n in s for n in needles)\n",
        "\n",
        "\n",
        "def _run_yt_dlp(cmd: list[str], log_path: Path) -> tuple[int, str]:\n",
        "    p = subprocess.run(cmd, capture_output=True, text=True)\n",
        "    combined = (p.stdout or '') + (p.stderr or '')\n",
        "    try:\n",
        "        # Fixed the SyntaxError here by using \\n instead of literal newlines\n",
        "        log_path.write_text('CMD:\\n' + ' '.join(cmd) + '\\n\\n' + combined, encoding='utf-8')\n",
        "    except Exception:\n",
        "        pass\n",
        "    return p.returncode, combined\n",
        "\n",
        "\n",
        "def _collect_urls(text: str) -> list[str]:\n",
        "    out = []\n",
        "    seen = set()\n",
        "    for line in (text or '').splitlines():\n",
        "        u = line.strip()\n",
        "        if not u or u.startswith('#'):\n",
        "            continue\n",
        "        if u not in seen:\n",
        "            out.append(u)\n",
        "            seen.add(u)\n",
        "    return out\n",
        "\n",
        "\n",
        "def _is_youtube(url: str) -> bool:\n",
        "    u = url.lower()\n",
        "    return 'youtube.com' in u or 'youtu.be' in u\n",
        "\n",
        "\n",
        "def download_one(url: str, out_dir: Path, cookie_file: str) -> dict:\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    base_cmd = [\n",
        "        sys.executable, '-m', 'yt_dlp',\n",
        "        '--no-playlist',\n",
        "        '--newline',\n",
        "        '--socket-timeout', '30',\n",
        "        '--retries', '15',\n",
        "        '--fragment-retries', '15',\n",
        "        '--extractor-retries', '5',\n",
        "        '--retry-sleep', 'http:exp=1:20',\n",
        "        '--http-chunk-size', '10M',\n",
        "        '-N', str(int(N_CONNECTIONS)),\n",
        "        '-f', 'bestaudio/best',\n",
        "        '-o', str(out_dir / '%(title).150s_%(id)s.%(ext)s'),\n",
        "    ]\n",
        "\n",
        "    if FORCE_IPV4:\n",
        "        base_cmd += ['--force-ipv4']\n",
        "\n",
        "    if cookie_file:\n",
        "        base_cmd += ['--cookies', cookie_file]\n",
        "\n",
        "    jsrt = _js_runtime_auto()\n",
        "    if jsrt:\n",
        "        base_cmd += ['--js-runtimes', jsrt]\n",
        "\n",
        "    def cmd_for(client: str | None):\n",
        "        cmd = list(base_cmd)\n",
        "        if _is_youtube(url) and client and client != 'default':\n",
        "            cmd += ['--extractor-args', f'youtube:player_client={client}']\n",
        "        if OUT_MODE == 'extract':\n",
        "            cmd += ['-x', '--audio-format', OUT_FMT]\n",
        "            if OUT_FMT.lower() in {'mp3','m4a','aac','opus','vorbis'}:\n",
        "                cmd += ['--audio-quality', str(int(OUT_QUALITY))]\n",
        "        cmd += [url]\n",
        "        return cmd\n",
        "\n",
        "    if PLAYER_CLIENT == 'auto':\n",
        "        clients = ['web,android', 'android', 'ios', 'tv_embedded,web', 'default']\n",
        "    else:\n",
        "        clients = [PLAYER_CLIENT]\n",
        "\n",
        "    last = {'url': url, 'ok': False, 'error': 'unknown'}\n",
        "\n",
        "    for attempt, client in enumerate(clients, 1):\n",
        "        log_path = DL_LOG_DIR / f'{_ts()}_{hashlib.md5(url.encode(\"utf-8\")).hexdigest()[:10]}_{attempt}_{safe_name(client)}.txt'\n",
        "        cmd = cmd_for(client)\n",
        "        if SHOW_CMD:\n",
        "            print('CMD:', ' '.join(cmd))\n",
        "        code, log = _run_yt_dlp(cmd, log_path)\n",
        "\n",
        "        if OUT_MODE == 'extract':\n",
        "            want_ext = '.' + OUT_FMT.lower()\n",
        "            files = sorted([p for p in out_dir.glob(f'*{want_ext}')], key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "        else:\n",
        "            files = sorted([p for p in out_dir.glob('*') if p.is_file() and p.suffix.lower() in AUDIO_EXTS], key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "\n",
        "        if code == 0 and files:\n",
        "            fp = files[0]\n",
        "            return {'url': url, 'ok': True, 'path': str(fp), 'client': client, 'log': str(log_path)}\n",
        "\n",
        "        last = {'url': url, 'ok': False, 'error': (log[-1000:] if log else f'exit={code}'), 'client': client, 'log': str(log_path)}\n",
        "\n",
        "        if not _looks_like_blocked(log):\n",
        "            break\n",
        "\n",
        "    return last\n",
        "\n",
        "\n",
        "if not DO_DOWNLOAD:\n",
        "    print('download skipped')\n",
        "else:\n",
        "    urls = _collect_urls(URLS)\n",
        "    if not urls:\n",
        "        print('URLS is empty')\n",
        "    else:\n",
        "        cookie_file = str(COOKIE_FILE) if USE_COOKIES and COOKIE_FILE.exists() else ''\n",
        "        log_event('download', f'urls={len(urls)} mode={OUT_MODE} fmt={OUT_FMT} q={OUT_QUALITY} par={URL_PARALLEL} N={N_CONNECTIONS} cookie={bool(cookie_file)}')\n",
        "\n",
        "        if DIAG_FORMATS_ONLY:\n",
        "            url = urls[0]\n",
        "            cmd = [sys.executable, '-m', 'yt_dlp', '--no-playlist']\n",
        "            if cookie_file:\n",
        "                cmd += ['--cookies', cookie_file]\n",
        "            jsrt = _js_runtime_auto()\n",
        "            if jsrt:\n",
        "                cmd += ['--js-runtimes', jsrt]\n",
        "            if FORCE_IPV4:\n",
        "                cmd += ['--force-ipv4']\n",
        "            cmd += ['-F', url]\n",
        "            lp = DL_LOG_DIR / f'{_ts()}_formats_{hashlib.md5(url.encode(\"utf-8\")).hexdigest()[:10]}.txt'\n",
        "            code, log = _run_yt_dlp(cmd, lp)\n",
        "            print('Exit=', code)\n",
        "            print('Saved:', lp)\n",
        "            print('\\n'.join((log or '').splitlines()[-80:]))\n",
        "        else:\n",
        "            from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "            batch_dir = AUDIO_DIR / f'youtube_{_ts()}'\n",
        "            batch_dir.mkdir(parents=True, exist_ok=True)\n",
        "            print('output:', batch_dir)\n",
        "\n",
        "            results = []\n",
        "            par = max(1, int(URL_PARALLEL))\n",
        "            if par <= 1:\n",
        "                for u in urls:\n",
        "                    r = download_one(u, batch_dir, cookie_file)\n",
        "                    results.append(r)\n",
        "                    print('OK' if r.get('ok') else 'FAIL', u)\n",
        "            else:\n",
        "                with ThreadPoolExecutor(max_workers=par) as pool:\n",
        "                    futs = {pool.submit(download_one, u, batch_dir, cookie_file): u for u in urls}\n",
        "                    for fut in as_completed(futs):\n",
        "                        r = fut.result()\n",
        "                        results.append(r)\n",
        "                        print('OK' if r.get('ok') else 'FAIL', r.get('url'))\n",
        "\n",
        "            ok = [r for r in results if r.get('ok')]\n",
        "            fail = [r for r in results if not r.get('ok')]\n",
        "            print(f'\\nDone: {len(ok)}/{len(results)} ok')\n",
        "            for r in ok:\n",
        "                print('  OK:', r.get('path'))\n",
        "            for r in fail[:10]:\n",
        "                print('  FAIL:', r.get('url'))\n",
        "                print('        client:', r.get('client'), 'log:', r.get('log'))\n",
        "\n",
        "            log_event('download', f'done ok={len(ok)} total={len(results)} out={batch_dir}')"
      ],
      "id": "1VuzJEf15PeD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "70b4zihN66rs"
      },
      "id": "70b4zihN66rs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK4snYcU5PeD",
        "outputId": "cf66000f-3967-4f34-ed38-24179417a48c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import skipped\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Step 1-ALT: Import External Audio (skip download)\n",
        "# ============================================================\n",
        "\n",
        "DO_IMPORT = False  #@param {type:\"boolean\"}\n",
        "IMPORT_MODE = 'upload'  #@param ['upload', 'path']\n",
        "IMPORT_PATH = ''  #@param {type:\"string\"}\n",
        "\n",
        "IMPORT_DIR = AUDIO_DIR / 'imported'\n",
        "IMPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def _copy_one(src: Path, dst_dir: Path) -> tuple[bool, str]:\n",
        "    if not src.exists() or not src.is_file():\n",
        "        return False, f'not_found: {src}'\n",
        "    if src.suffix.lower() not in AUDIO_EXTS:\n",
        "        return False, f'unsupported: {src.name}'\n",
        "    dst = dst_dir / src.name\n",
        "    if dst.exists():\n",
        "        return False, f'exists: {dst.name}'\n",
        "    shutil.copy2(src, dst)\n",
        "    return True, dst.name\n",
        "\n",
        "\n",
        "if not DO_IMPORT:\n",
        "    print('import skipped')\n",
        "else:\n",
        "    from google.colab import files\n",
        "\n",
        "    imported = []\n",
        "    skipped = []\n",
        "\n",
        "    if IMPORT_MODE == 'upload':\n",
        "        up = files.upload()\n",
        "        if not up:\n",
        "            print('no file uploaded')\n",
        "        for name, data in up.items():\n",
        "            p = IMPORT_DIR / Path(name).name\n",
        "            if p.suffix.lower() not in AUDIO_EXTS:\n",
        "                skipped.append(f'unsupported: {p.name}')\n",
        "                continue\n",
        "            p.write_bytes(data)\n",
        "            imported.append(p.name)\n",
        "\n",
        "    else:\n",
        "        p = IMPORT_PATH.strip()\n",
        "        if not p:\n",
        "            print('IMPORT_PATH empty')\n",
        "        else:\n",
        "            src = Path(p)\n",
        "            if src.exists() and src.is_file() and src.suffix.lower() == '.zip':\n",
        "                import zipfile\n",
        "                with zipfile.ZipFile(src, 'r') as zf:\n",
        "                    zf.extractall(IMPORT_DIR)\n",
        "                imported.append(f'zip_extract: {src.name}')\n",
        "            elif src.exists() and src.is_file():\n",
        "                ok, msg = _copy_one(src, IMPORT_DIR)\n",
        "                (imported if ok else skipped).append(msg)\n",
        "            elif src.exists() and src.is_dir():\n",
        "                files2 = sorted([f for f in src.rglob('*') if f.is_file() and f.suffix.lower() in AUDIO_EXTS])\n",
        "                for f in files2:\n",
        "                    ok, msg = _copy_one(f, IMPORT_DIR)\n",
        "                    (imported if ok else skipped).append(msg)\n",
        "            else:\n",
        "                matches = sorted(glob.glob(p, recursive=True))\n",
        "                if matches:\n",
        "                    for m in matches:\n",
        "                        ok, msg = _copy_one(Path(m), IMPORT_DIR)\n",
        "                        (imported if ok else skipped).append(msg)\n",
        "                else:\n",
        "                    skipped.append(f'not_found: {p}')\n",
        "\n",
        "    print(f'imported: {len(imported)}')\n",
        "    for n in imported[:50]:\n",
        "        print('  +', n)\n",
        "    if skipped:\n",
        "        print('skipped:')\n",
        "        for s in skipped[:50]:\n",
        "            print('  -', s)\n",
        "\n",
        "    log_event('import', f'done imported={len(imported)} skipped={len(skipped)} dir={IMPORT_DIR}')"
      ],
      "id": "aK4snYcU5PeD"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f3253478ae064e16a0d7ca626d2e805f",
            "067d4e4571094f0281dc58f7135888aa",
            "7ee86e0168e7476e9b454c14336988ab",
            "d8c686fe982547c2984f24a48e05cdd0",
            "910ef36e6b4949ddb7a2eba01e9182bb",
            "430e4c91174a4767ba6179e98c565d47",
            "100b0132360a4561a3427bc9d0ad0694",
            "023739cb652b46e8b17c610bbb31605d",
            "6885d2dc80404eb9b19cc7128eeff89f",
            "6cf584ef2f0b455d8d3ea70b7dbd1f27",
            "c27b9f0e06b542afb32c28fb2172158f"
          ]
        },
        "id": "a1fQVOpO5PeE",
        "outputId": "7672cd2e-691a-4452-86d4-a3813486cbe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2026-02-11 04:37:12] [INFO] [transcribe] files=1 model=small lang=en task=transcribe beam=5 temp=0.0\n",
            "GPU: 1 | use_multi=False | workers=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 461M/461M [00:01<00:00, 365MiB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Transcribing:   0%|          | 0/1 [00:00<?, ?file/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3253478ae064e16a0d7ca626d2e805f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/325632 [00:00<?, ?frames/s]\u001b[A\n",
            "  1%|          | 2448/325632 [00:03<08:14, 653.59frames/s]\u001b[A\n",
            "  1%|▏         | 4408/325632 [00:06<07:20, 728.83frames/s]\u001b[A\n",
            "  2%|▏         | 6864/325632 [00:08<05:45, 922.90frames/s]\u001b[A\n",
            "  3%|▎         | 9384/325632 [00:10<05:19, 991.13frames/s]\u001b[A\n",
            "  4%|▎         | 11608/325632 [00:12<04:57, 1055.31frames/s]\u001b[A\n",
            "  4%|▍         | 14176/325632 [00:14<05:00, 1035.79frames/s]\u001b[A\n",
            "  5%|▌         | 16488/325632 [00:17<05:11, 991.89frames/s] \u001b[A\n",
            "  6%|▌         | 19104/325632 [00:19<05:00, 1019.85frames/s]\u001b[A\n",
            "  7%|▋         | 21736/325632 [00:22<05:16, 960.51frames/s] \u001b[A\n",
            "  7%|▋         | 24344/325632 [00:25<04:57, 1011.17frames/s]\u001b[A\n",
            "  8%|▊         | 27216/325632 [00:27<04:45, 1045.90frames/s]\u001b[A\n",
            "  9%|▉         | 30048/325632 [00:30<04:41, 1048.79frames/s]\u001b[A\n",
            " 10%|█         | 32712/325632 [00:32<04:24, 1107.33frames/s]\u001b[A\n",
            " 11%|█         | 34704/325632 [00:34<04:27, 1089.18frames/s]\u001b[A\n",
            " 12%|█▏        | 37600/325632 [00:36<04:19, 1109.87frames/s]\u001b[A\n",
            " 12%|█▏        | 40208/325632 [00:39<04:20, 1095.82frames/s]\u001b[A\n",
            " 13%|█▎        | 42592/325632 [00:42<04:43, 997.27frames/s] \u001b[A\n",
            " 14%|█▍        | 45032/325632 [00:44<04:34, 1022.69frames/s]\u001b[A\n",
            " 15%|█▍        | 47752/325632 [00:46<04:04, 1135.78frames/s]\u001b[A\n",
            " 16%|█▌        | 50712/325632 [00:48<03:52, 1183.53frames/s]\u001b[A\n",
            " 16%|█▋        | 53224/325632 [00:51<03:59, 1139.20frames/s]\u001b[A\n",
            " 17%|█▋        | 55672/325632 [00:53<04:22, 1026.67frames/s]\u001b[A\n",
            " 18%|█▊        | 58360/325632 [00:56<04:14, 1051.14frames/s]\u001b[A\n",
            " 19%|█▉        | 61096/325632 [00:58<04:04, 1082.30frames/s]\u001b[A\n",
            " 20%|█▉        | 63920/325632 [01:01<03:58, 1097.54frames/s]\u001b[A\n",
            " 20%|██        | 66248/325632 [01:03<04:06, 1050.29frames/s]\u001b[A\n",
            " 21%|██        | 68688/325632 [01:05<03:52, 1105.73frames/s]\u001b[A\n",
            " 22%|██▏       | 71160/325632 [01:08<04:02, 1049.95frames/s]\u001b[A\n",
            " 23%|██▎       | 73960/325632 [01:10<03:53, 1075.97frames/s]\u001b[A\n",
            " 23%|██▎       | 76416/325632 [01:13<04:00, 1037.24frames/s]\u001b[A\n",
            " 24%|██▍       | 79240/325632 [01:15<03:43, 1100.11frames/s]\u001b[A\n",
            " 25%|██▌       | 81408/325632 [01:17<03:34, 1141.02frames/s]\u001b[A\n",
            " 26%|██▌       | 83992/325632 [01:19<03:24, 1182.16frames/s]\u001b[A\n",
            " 27%|██▋       | 86584/325632 [01:21<03:20, 1190.12frames/s]\u001b[A\n",
            " 27%|██▋       | 88968/325632 [01:23<03:10, 1241.74frames/s]\u001b[A\n",
            " 28%|██▊       | 91736/325632 [01:25<03:10, 1226.09frames/s]\u001b[A\n",
            " 29%|██▉       | 94112/325632 [01:27<03:23, 1140.16frames/s]\u001b[A\n",
            " 30%|██▉       | 96696/325632 [01:31<03:52, 986.72frames/s] \u001b[A\n",
            " 30%|███       | 99064/325632 [01:33<03:48, 990.54frames/s]\u001b[A\n",
            " 31%|███       | 100912/325632 [01:35<03:50, 973.00frames/s]\u001b[A\n",
            " 32%|███▏      | 102984/325632 [01:37<03:40, 1011.80frames/s]\u001b[A\n",
            " 32%|███▏      | 105200/325632 [01:39<03:29, 1050.58frames/s]\u001b[A\n",
            " 33%|███▎      | 107904/325632 [01:41<03:22, 1075.95frames/s]\u001b[A\n",
            " 34%|███▍      | 110872/325632 [01:44<03:19, 1076.72frames/s]\u001b[A\n",
            " 35%|███▍      | 113536/325632 [01:47<03:22, 1046.00frames/s]\u001b[A\n",
            " 35%|███▌      | 115584/325632 [01:49<03:24, 1026.69frames/s]\u001b[A\n",
            " 36%|███▋      | 118160/325632 [01:51<03:06, 1110.90frames/s]\u001b[A\n",
            " 37%|███▋      | 120936/325632 [01:53<03:05, 1103.44frames/s]\u001b[A\n",
            " 38%|███▊      | 123096/325632 [01:56<03:13, 1047.33frames/s]\u001b[A\n",
            " 39%|███▊      | 125568/325632 [01:58<03:11, 1043.95frames/s]\u001b[A\n",
            " 39%|███▉      | 128216/325632 [02:01<03:20, 986.93frames/s] \u001b[A\n",
            " 40%|████      | 130776/325632 [02:03<03:10, 1020.57frames/s]\u001b[A\n",
            " 41%|████      | 132880/325632 [02:05<02:49, 1136.34frames/s]\u001b[A\n",
            " 42%|████▏     | 135848/325632 [02:07<02:37, 1204.54frames/s]\u001b[A\n",
            " 42%|████▏     | 137864/325632 [02:09<02:47, 1120.26frames/s]\u001b[A\n",
            " 43%|████▎     | 139984/325632 [02:11<02:39, 1163.01frames/s]\u001b[A\n",
            " 44%|████▍     | 142488/325632 [02:13<02:32, 1204.47frames/s]\u001b[A\n",
            " 45%|████▍     | 145208/325632 [02:15<02:31, 1190.37frames/s]\u001b[A\n",
            " 45%|████▌     | 148056/325632 [02:18<02:36, 1136.00frames/s]\u001b[A\n",
            " 46%|████▌     | 150448/325632 [02:20<02:34, 1136.75frames/s]\u001b[A\n",
            " 47%|████▋     | 153232/325632 [02:22<02:24, 1191.99frames/s]\u001b[A\n",
            " 48%|████▊     | 156232/325632 [02:24<02:13, 1265.71frames/s]\u001b[A\n",
            " 49%|████▊     | 158640/325632 [02:25<01:55, 1444.80frames/s]\u001b[A\n",
            " 50%|████▉     | 161416/325632 [02:27<01:58, 1385.39frames/s]\u001b[A\n",
            " 50%|█████     | 163840/325632 [02:29<01:59, 1351.48frames/s]\u001b[A\n",
            " 51%|█████     | 166152/325632 [02:31<01:57, 1359.47frames/s]\u001b[A\n",
            " 52%|█████▏    | 168736/325632 [02:33<02:06, 1236.77frames/s]\u001b[A\n",
            " 53%|█████▎    | 171624/325632 [02:35<02:00, 1281.95frames/s]\u001b[A\n",
            " 54%|█████▎    | 174512/325632 [02:38<01:58, 1277.97frames/s]\u001b[A\n",
            " 54%|█████▍    | 177200/325632 [02:40<02:01, 1221.58frames/s]\u001b[A\n",
            " 55%|█████▌    | 180040/325632 [02:43<02:11, 1104.71frames/s]\u001b[A\n",
            " 56%|█████▌    | 182696/325632 [02:46<02:10, 1094.30frames/s]\u001b[A\n",
            " 57%|█████▋    | 185320/325632 [02:48<02:10, 1076.20frames/s]\u001b[A\n",
            " 58%|█████▊    | 188160/325632 [02:51<02:09, 1063.55frames/s]\u001b[A\n",
            " 58%|█████▊    | 190336/325632 [02:53<02:13, 1015.40frames/s]\u001b[A\n",
            " 59%|█████▉    | 193224/325632 [02:56<02:11, 1005.35frames/s]\u001b[A\n",
            " 60%|██████    | 195600/325632 [02:59<02:06, 1025.28frames/s]\u001b[A\n",
            " 61%|██████    | 198304/325632 [03:00<01:53, 1120.59frames/s]\u001b[A\n",
            " 62%|██████▏   | 200416/325632 [03:02<01:46, 1170.63frames/s]\u001b[A\n",
            " 62%|██████▏   | 202880/325632 [03:04<01:38, 1250.18frames/s]\u001b[A\n",
            " 63%|██████▎   | 205696/325632 [03:06<01:30, 1325.88frames/s]\u001b[A\n",
            " 64%|██████▍   | 207840/325632 [03:08<01:38, 1192.43frames/s]\u001b[A\n",
            " 65%|██████▍   | 210384/325632 [03:10<01:31, 1254.02frames/s]\u001b[A\n",
            " 65%|██████▌   | 212816/325632 [03:12<01:39, 1130.16frames/s]\u001b[A\n",
            " 66%|██████▌   | 214528/325632 [03:14<01:39, 1121.46frames/s]\u001b[A\n",
            " 67%|██████▋   | 216800/325632 [03:15<01:22, 1313.28frames/s]\u001b[A\n",
            " 67%|██████▋   | 219784/325632 [03:17<01:14, 1429.18frames/s]\u001b[A\n",
            " 68%|██████▊   | 222496/325632 [03:19<01:18, 1318.79frames/s]\u001b[A\n",
            " 69%|██████▉   | 225160/325632 [03:21<01:11, 1402.09frames/s]\u001b[A\n",
            " 70%|██████▉   | 227512/325632 [03:22<01:10, 1401.65frames/s]\u001b[A\n",
            " 71%|███████   | 230128/325632 [03:24<01:06, 1425.44frames/s]\u001b[A\n",
            " 71%|███████▏  | 232472/325632 [03:26<01:04, 1447.75frames/s]\u001b[A\n",
            " 72%|███████▏  | 235344/325632 [03:28<01:06, 1366.36frames/s]\u001b[A\n",
            " 73%|███████▎  | 238336/325632 [03:30<01:04, 1345.39frames/s]\u001b[A\n",
            " 74%|███████▍  | 240616/325632 [03:32<01:02, 1350.10frames/s]\u001b[A\n",
            " 75%|███████▍  | 243296/325632 [03:34<01:02, 1313.42frames/s]\u001b[A\n",
            " 76%|███████▌  | 245952/325632 [03:36<00:57, 1386.86frames/s]\u001b[A\n",
            " 76%|███████▋  | 248680/325632 [03:38<00:54, 1413.35frames/s]\u001b[A\n",
            " 77%|███████▋  | 251576/325632 [03:40<00:55, 1337.35frames/s]\u001b[A\n",
            " 78%|███████▊  | 254096/325632 [03:43<00:58, 1216.18frames/s]\u001b[A\n",
            " 79%|███████▊  | 256376/325632 [03:45<00:57, 1204.55frames/s]\u001b[A\n",
            " 79%|███████▉  | 258448/325632 [03:46<00:54, 1229.07frames/s]\u001b[A\n",
            " 80%|████████  | 261408/325632 [03:48<00:49, 1304.81frames/s]\u001b[A\n",
            " 81%|████████  | 263208/325632 [03:50<00:50, 1234.77frames/s]\u001b[A\n",
            " 82%|████████▏ | 265864/325632 [03:52<00:45, 1314.27frames/s]\u001b[A\n",
            " 82%|████████▏ | 268200/325632 [03:53<00:42, 1353.87frames/s]\u001b[A\n",
            " 83%|████████▎ | 270872/325632 [03:55<00:38, 1438.14frames/s]\u001b[A\n",
            " 84%|████████▍ | 273480/325632 [03:56<00:35, 1478.48frames/s]\u001b[A\n",
            " 85%|████████▍ | 276080/325632 [03:58<00:29, 1666.41frames/s]\u001b[A\n",
            " 86%|████████▌ | 278576/325632 [04:00<00:31, 1514.41frames/s]\u001b[A\n",
            " 86%|████████▋ | 281088/325632 [04:02<00:32, 1358.73frames/s]\u001b[A\n",
            " 87%|████████▋ | 283592/325632 [04:04<00:33, 1238.85frames/s]\u001b[A\n",
            " 88%|████████▊ | 286280/325632 [04:06<00:31, 1246.64frames/s]\u001b[A\n",
            " 89%|████████▉ | 289264/325632 [04:10<00:31, 1136.79frames/s]\u001b[A\n",
            " 89%|████████▉ | 291368/325632 [04:12<00:30, 1105.35frames/s]\u001b[A\n",
            " 90%|█████████ | 294144/325632 [04:13<00:25, 1216.28frames/s]\u001b[A\n",
            " 91%|█████████ | 296288/325632 [04:15<00:22, 1292.68frames/s]\u001b[A\n",
            " 92%|█████████▏| 299288/325632 [04:17<00:19, 1340.72frames/s]\u001b[A\n",
            " 93%|█████████▎| 301888/325632 [04:19<00:18, 1267.68frames/s]\u001b[A\n",
            " 93%|█████████▎| 304120/325632 [04:22<00:18, 1152.00frames/s]\u001b[A\n",
            " 94%|█████████▍| 306584/325632 [04:24<00:16, 1159.61frames/s]\u001b[A\n",
            " 95%|█████████▍| 308712/325632 [04:26<00:15, 1116.80frames/s]\u001b[A\n",
            " 96%|█████████▌| 311200/325632 [04:28<00:13, 1107.67frames/s]\u001b[A\n",
            " 96%|█████████▋| 313472/325632 [04:31<00:11, 1028.12frames/s]\u001b[A\n",
            " 97%|█████████▋| 316472/325632 [04:33<00:07, 1169.98frames/s]\u001b[A\n",
            " 98%|█████████▊| 319384/325632 [04:34<00:04, 1268.20frames/s]\u001b[A\n",
            " 99%|█████████▉| 321840/325632 [04:36<00:02, 1318.80frames/s]\u001b[A\n",
            "100%|█████████▉| 324840/325632 [04:37<00:00, 1170.16frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2026-02-11 04:42:06] [INFO] [transcribe] OK file=Joe Tsai, Co-Founder and Chairman, Alibaba： Find Your People_dbV1sMfv9TQ.mp4 seconds=286.1 lang=en\n",
            "done. ok= 1 total= 1\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Step 2: Whisper Batch Transcribe\n",
        "# ============================================================\n",
        "\n",
        "DO_TRANSCRIBE = True  #@param {type:\"boolean\"}\n",
        "\n",
        "MODEL_NAME = 'small'  #@param ['tiny', 'base', 'small', 'medium', 'large']\n",
        "LANGUAGE = 'en'  #@param ['auto', 'zh', 'en', 'ja', 'ko', 'es', 'fr', 'de']\n",
        "TASK = 'transcribe'  #@param ['transcribe', 'translate']\n",
        "BEAM_SIZE = 5  #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "TEMPERATURE = 0.0  #@param {type:\"slider\", min:0.0, max:1.0, step:0.1}\n",
        "\n",
        "USE_MULTI_GPU = False  #@param {type:\"boolean\"}\n",
        "GPU_WORKERS = 1  #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "def _list_audio() -> list[Path]:\n",
        "    return sorted([p for p in AUDIO_DIR.rglob('*') if p.is_file() and p.suffix.lower() in AUDIO_EXTS])\n",
        "\n",
        "\n",
        "def _write_outputs(result: dict, audio_path: Path, out_root: Path):\n",
        "    import json as _json\n",
        "    from whisper.utils import get_writer\n",
        "\n",
        "    base = safe_name(audio_path.stem)\n",
        "    one_out = out_root / base\n",
        "    one_out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    get_writer('txt', str(one_out))(result, base)\n",
        "    get_writer('srt', str(one_out))(result, base)\n",
        "\n",
        "    (one_out / f'{base}.json').write_text(_json.dumps(result, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "\n",
        "\n",
        "if not DO_TRANSCRIBE:\n",
        "    print('transcribe skipped')\n",
        "else:\n",
        "    audio_files = _list_audio()\n",
        "    if not audio_files:\n",
        "        print('No audio found. Run Step 1 or Step 1-ALT first.')\n",
        "        raise SystemExit(1)\n",
        "\n",
        "    log_event('transcribe', f'files={len(audio_files)} model={MODEL_NAME} lang={LANGUAGE} task={TASK} beam={BEAM_SIZE} temp={TEMPERATURE}')\n",
        "\n",
        "    n_gpus = torch.cuda.device_count()\n",
        "    use_multi = bool(USE_MULTI_GPU) and n_gpus > 1\n",
        "    workers = max(1, int(GPU_WORKERS))\n",
        "    if use_multi:\n",
        "        workers = min(workers, n_gpus, len(audio_files))\n",
        "    else:\n",
        "        workers = 1\n",
        "\n",
        "    print(f'GPU: {n_gpus} | use_multi={use_multi} | workers={workers}')\n",
        "\n",
        "    if workers == 1:\n",
        "        import whisper\n",
        "        from tqdm.auto import tqdm\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        model = whisper.load_model(MODEL_NAME, device=device)\n",
        "        lang_arg = None if LANGUAGE == 'auto' else LANGUAGE\n",
        "\n",
        "        ok = 0\n",
        "        for ap in tqdm(audio_files, desc='Transcribing', unit='file'):\n",
        "            base = safe_name(ap.stem)\n",
        "            out_dir = OUT_DIR / base\n",
        "            out_json = out_dir / f'{base}.json'\n",
        "            if out_json.exists():\n",
        "                continue\n",
        "\n",
        "            t0 = time.time()\n",
        "            try:\n",
        "                result = model.transcribe(\n",
        "                    str(ap),\n",
        "                    language=lang_arg,\n",
        "                    task=TASK,\n",
        "                    beam_size=int(BEAM_SIZE),\n",
        "                    temperature=float(TEMPERATURE),\n",
        "                    fp16=(device == 'cuda'),\n",
        "                    verbose=False,\n",
        "                )\n",
        "                _write_outputs(result, ap, OUT_DIR)\n",
        "                ok += 1\n",
        "                log_event('transcribe', f'OK file={ap.name} seconds={time.time()-t0:.1f} lang={result.get(\"language\")}')\n",
        "            except Exception as e:\n",
        "                log_exception('transcribe', f'file={ap.name}', e)\n",
        "\n",
        "        print('done. ok=', ok, 'total=', len(audio_files))\n",
        "\n",
        "    else:\n",
        "        import textwrap as _tw\n",
        "        from tqdm.auto import tqdm\n",
        "\n",
        "        worker_py = RUN_DIR / '_whisper_worker.py'\n",
        "        worker_code = r'''\n",
        "import argparse, json, os, re, sys, time\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import whisper\n",
        "from whisper.utils import get_writer\n",
        "\n",
        "def safe_name(name):\n",
        "    name = re.sub(r'[\\/:*?\"<>|]+', '_', name)\n",
        "    return re.sub(r'\\s+', ' ', name).strip()[:180]\n",
        "\n",
        "def write_outputs(result: dict, audio_path: Path, out_root: Path):\n",
        "    base = safe_name(audio_path.stem)\n",
        "    one_out = out_root / base\n",
        "    one_out.mkdir(parents=True, exist_ok=True)\n",
        "    get_writer('txt', str(one_out))(result, base)\n",
        "    get_writer('srt', str(one_out))(result, base)\n",
        "    (one_out / f'{base}.json').write_text(json.dumps(result, ensure_ascii=False, indent=2), encoding='utf-8')\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument('--files-json', required=True)\n",
        "    ap.add_argument('--out-dir', required=True)\n",
        "    ap.add_argument('--model', required=True)\n",
        "    ap.add_argument('--language', default='auto')\n",
        "    ap.add_argument('--task', default='transcribe')\n",
        "    ap.add_argument('--beam-size', type=int, default=5)\n",
        "    ap.add_argument('--temperature', type=float, default=0.0)\n",
        "    ap.add_argument('--label', default='')\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    out_dir = Path(args.out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    files = json.loads(Path(args.files_json).read_text('utf-8'))\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    label = args.label or device\n",
        "\n",
        "    print(f'[{label}] loading {args.model} on {device}, {len(files)} files')\n",
        "    sys.stdout.flush()\n",
        "    model = whisper.load_model(args.model, device=device)\n",
        "    print(f'[{label}] model loaded')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    lang_arg = None if args.language == 'auto' else args.language\n",
        "\n",
        "    for idx, fpath in enumerate(files, 1):\n",
        "        apath = Path(fpath)\n",
        "        base = safe_name(apath.stem)\n",
        "        out_json = out_dir / base / f'{base}.json'\n",
        "        if out_json.exists():\n",
        "            print(f'[{label}] ({idx}/{len(files)}) SKIP {apath.name}')\n",
        "            continue\n",
        "\n",
        "        print(f'[{label}] ({idx}/{len(files)}) transcribing {apath.name}')\n",
        "        sys.stdout.flush()\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            result = model.transcribe(\n",
        "                str(apath),\n",
        "                language=lang_arg,\n",
        "                task=args.task,\n",
        "                beam_size=args.beam_size,\n",
        "                temperature=args.temperature,\n",
        "                fp16=(device == 'cuda'),\n",
        "                verbose=False,\n",
        "            )\n",
        "            write_outputs(result, apath, out_dir)\n",
        "            print(f'[{label}] ({idx}/{len(files)}) DONE {time.time()-t0:.1f}s')\n",
        "        except Exception as e:\n",
        "            print(f'[{label}] ({idx}/{len(files)}) FAIL {time.time()-t0:.1f}s: {e}')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    return 0\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    raise SystemExit(main())\n",
        "'''\n",
        "        worker_py.write_text(_tw.dedent(worker_code), encoding='utf-8')\n",
        "\n",
        "        chunks = [[] for _ in range(workers)]\n",
        "        for i, ap in enumerate(audio_files):\n",
        "            chunks[i % workers].append(str(ap))\n",
        "\n",
        "        procs = []\n",
        "        log_paths = []\n",
        "        t_start = time.time()\n",
        "\n",
        "        for slot in range(workers):\n",
        "            files_json = RUN_DIR / f'files_gpu{slot}.json'\n",
        "            files_json.write_text(json.dumps(chunks[slot], ensure_ascii=False), encoding='utf-8')\n",
        "            lp = RUN_DIR / f'log_gpu{slot}.txt'\n",
        "            log_paths.append(lp)\n",
        "\n",
        "            env = os.environ.copy()\n",
        "            env['CUDA_VISIBLE_DEVICES'] = str(slot)\n",
        "\n",
        "            cmd = [\n",
        "                sys.executable, str(worker_py),\n",
        "                '--files-json', str(files_json),\n",
        "                '--out-dir', str(OUT_DIR),\n",
        "                '--model', MODEL_NAME,\n",
        "                '--language', LANGUAGE,\n",
        "                '--task', TASK,\n",
        "                '--beam-size', str(int(BEAM_SIZE)),\n",
        "                '--temperature', str(float(TEMPERATURE)),\n",
        "                '--label', f'gpu{slot}',\n",
        "            ]\n",
        "\n",
        "            f = open(lp, 'w', encoding='utf-8')\n",
        "            procs.append((subprocess.Popen(cmd, env=env, stdout=f, stderr=subprocess.STDOUT), f))\n",
        "\n",
        "        def count_done():\n",
        "            n = 0\n",
        "            for p in OUT_DIR.rglob('*.json'):\n",
        "                if p.name.startswith('manifest_'):\n",
        "                    continue\n",
        "                n += 1\n",
        "            return n\n",
        "\n",
        "        pbar = tqdm(total=len(audio_files), desc='Transcribing', unit='file')\n",
        "        last = count_done()\n",
        "        pbar.update(min(last, pbar.total))\n",
        "\n",
        "        while any(proc.poll() is None for proc, _f in procs):\n",
        "            time.sleep(2)\n",
        "            cur = count_done()\n",
        "            if cur > last:\n",
        "                pbar.update(cur - last)\n",
        "                last = cur\n",
        "\n",
        "        exit_codes = []\n",
        "        for proc, f in procs:\n",
        "            exit_codes.append(proc.wait())\n",
        "            f.close()\n",
        "\n",
        "        cur = count_done()\n",
        "        if cur > last:\n",
        "            pbar.update(cur - last)\n",
        "        pbar.close()\n",
        "\n",
        "        log_event('transcribe', f'done seconds={time.time()-t_start:.1f} exit_codes={exit_codes} done={cur}/{len(audio_files)}')\n",
        "\n",
        "        for lp in log_paths:\n",
        "            if lp.exists():\n",
        "                print(f'\\n--- {lp.name} ---')\n",
        "                txt = lp.read_text(encoding='utf-8', errors='replace').splitlines()\n",
        "                print('\\n'.join(txt[-40:]))"
      ],
      "id": "a1fQVOpO5PeE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "aRxZ2ozT5PeE",
        "outputId": "d7b6f328-b0aa-4659-f468-8f4efd9954e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2026-02-11 04:42:37] [INFO] [package] start\n",
            "[2026-02-11 04:42:37] [INFO] [package] zip=/content/whisperf/runs/20260211_042412/whisperf_run_20260211_042412.zip size_mb=0.1 files=5\n",
            "zip: /content/whisperf/runs/20260211_042412/whisperf_run_20260211_042412.zip (0.1 MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9c077901-ca26-40f2-9c2f-bba6fe64fdf9\", \"whisperf_run_20260211_042412.zip\", 111511)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Step 3: Package + Download (zip)\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "log_event('package', 'start')\n",
        "\n",
        "zip_name = f'whisperf_run_{RUN_ID}.zip'\n",
        "zip_path = RUN_DIR / zip_name\n",
        "\n",
        "payload = []\n",
        "payload += [p for p in OUT_DIR.rglob('*') if p.is_file()]\n",
        "if TASK_LOG_PATH.exists():\n",
        "    payload.append(TASK_LOG_PATH)\n",
        "payload += [p for p in DL_LOG_DIR.rglob('*.txt') if p.is_file()]\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
        "    for fp in payload:\n",
        "        try:\n",
        "            if fp == TASK_LOG_PATH:\n",
        "                zf.write(fp, arcname='task_log.txt')\n",
        "            elif str(fp).startswith(str(OUT_DIR)):\n",
        "                zf.write(fp, arcname=str(fp.relative_to(OUT_DIR)))\n",
        "            elif str(fp).startswith(str(DL_LOG_DIR)):\n",
        "                zf.write(fp, arcname=str(Path('download_logs') / fp.name))\n",
        "            else:\n",
        "                zf.write(fp, arcname=fp.name)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "size_mb = zip_path.stat().st_size / (1024 * 1024)\n",
        "log_event('package', f'zip={zip_path} size_mb={size_mb:.1f} files={len(payload)}')\n",
        "print('zip:', zip_path, f'({size_mb:.1f} MB)')\n",
        "\n",
        "files.download(str(zip_path))\n",
        "\n",
        "if DRIVE_RUN_DIR is not None:\n",
        "    dst = DRIVE_RUN_DIR / zip_name\n",
        "    shutil.copy2(zip_path, dst)\n",
        "    print('saved to drive:', dst)\n"
      ],
      "id": "aRxZ2ozT5PeE"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "history_visible": true,
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f3253478ae064e16a0d7ca626d2e805f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_067d4e4571094f0281dc58f7135888aa",
              "IPY_MODEL_7ee86e0168e7476e9b454c14336988ab",
              "IPY_MODEL_d8c686fe982547c2984f24a48e05cdd0"
            ],
            "layout": "IPY_MODEL_910ef36e6b4949ddb7a2eba01e9182bb"
          }
        },
        "067d4e4571094f0281dc58f7135888aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_430e4c91174a4767ba6179e98c565d47",
            "placeholder": "​",
            "style": "IPY_MODEL_100b0132360a4561a3427bc9d0ad0694",
            "value": "Transcribing: 100%"
          }
        },
        "7ee86e0168e7476e9b454c14336988ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_023739cb652b46e8b17c610bbb31605d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6885d2dc80404eb9b19cc7128eeff89f",
            "value": 1
          }
        },
        "d8c686fe982547c2984f24a48e05cdd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cf584ef2f0b455d8d3ea70b7dbd1f27",
            "placeholder": "​",
            "style": "IPY_MODEL_c27b9f0e06b542afb32c28fb2172158f",
            "value": " 1/1 [04:46&lt;00:00, 286.12s/file]"
          }
        },
        "910ef36e6b4949ddb7a2eba01e9182bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "430e4c91174a4767ba6179e98c565d47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "100b0132360a4561a3427bc9d0ad0694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "023739cb652b46e8b17c610bbb31605d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6885d2dc80404eb9b19cc7128eeff89f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cf584ef2f0b455d8d3ea70b7dbd1f27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c27b9f0e06b542afb32c28fb2172158f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}